{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning: Chapter 2\n",
    "\n",
    "\n",
    "Example:  Classify handwritten digits\n",
    "\n",
    "* grayscale images, 28x28 pixels into 10 categories\n",
    "* [MNISt](https://en.wikipedia.org/wiki/MNIST_database) dataset: 60k training images, 10k test images\n",
    "\n",
    "\n",
    "Terminology:\n",
    "\n",
    "* **Class**: a category in a classification problem \n",
    "* **Sample**: datapoints\n",
    "* **Label** class associatied with a specific sample\n",
    "\n",
    "MNIST database is preloaded in KERASin set of four Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `train_images` and `train_labels` frm the training set\n",
    "* `test_images` and `test_labels` is test set on which model will be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train_image = (60000, 28, 28)\n",
      "Length train_labels = 60000\n",
      "Shape test_images= (10000, 28, 28)\n",
      "Length test_labels = 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape train_image = {train_images.shape}\")\n",
    "print(f\"Length train_labels = {len(train_labels)}\")\n",
    "print(f\"Shape test_images= {test_images.shape}\")\n",
    "print(f\"Length test_labels = {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Workflow:*** \n",
    "\n",
    "1. feed neuronal network training data\n",
    "2. associate images with labels\n",
    "3. produce predictions for test images\n",
    "4. verify whether predictions match test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation = 'relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* layer: is a filter\n",
    "* layer extract representations of data fed into them.\n",
    "* layers are chained\n",
    "\n",
    "Network above consists of a sequence of two dense layers that are *densely* connected (or *fully* connected) neural layers. The second layer is a 10 way *softmax* layer which rturns 10 probability scores (summing to 1). Each score will give probability that current digit belongs to one of the 10 digit classes.\n",
    "\n",
    "Make network ready for training need to pick three more things as part of the *compilation* step: \n",
    "\n",
    "* optimizer (method to update network)\n",
    "* Loss function (how network measures performance on training data)\n",
    "* Metrics to monitor during training and testing (here: accuracy of classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reshape images into a `float32` array of shape (60000, 28*28)\n",
    "* rescale 8 bit images into values in interval `[0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 *28))\n",
    "test_images = test_images.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorically encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train network.  Call network's fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/clemenskaminski/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.2571 - accuracy: 0.9245\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1033 - accuracy: 0.9682\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0686 - accuracy: 0.9797\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0496 - accuracy: 0.9848\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0377 - accuracy: 0.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x106f0f190>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs =5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/step\n",
      "test_acc: 0.9819999933242798\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is lower than the training set accuracy.  This is an example of *overfitting*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data representations for neural networks\n",
    "\n",
    "Tensors:\n",
    "\n",
    "* rank 0: scalar `0`\n",
    "* rank 1: vector `[1,4,5,7]`\n",
    "* rank 2: array  `[[11,12,13], [21, 22, 23]]`\n",
    "* rank n: has n axes\n",
    "\n",
    "note: vector `[1,4,5,7]` has 4 dimensions, but is a tensor of rank 1 (often one speaks, inaccurately, of a one dimensional tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of data:\n",
    "\n",
    "* Vector data: e.g. dataset of dext documents where each is represented by the counts of how many times each word in a dictionary appears in it.  e.g. tensor of shape `(500, 20000)` might describe 500 documents with dictionary of 20000 words.\n",
    "\n",
    "* Time series data: E.g.stock prices as a tensor of rank 3: 250 days with values every minute (390 working minutes per day), recording low, high and current price for each minute `(250, 390, 3)`\n",
    "\n",
    "* Image data: tensor of rank 4: e.g. 128 images, 256 x 256 pixels, 3 colour channels. `(128, 256, 256, 3)`. note Tensorflow convention for order is `(samples, height, width, color_depth)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor attributes\n",
    "\n",
    "* Number of axes (rank)\n",
    "* shape: dimension of tensor along each axis\n",
    "* Data type (usually `dtype` in python libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "Means stretching dimensions of lower rank tensor to those of higher rank tensor for efficient computation.\n",
    "\n",
    "See [broadcasting article](https://numpy.org/devdocs/user/theory.broadcasting.html)\n",
    "\n",
    "* works from last axis backward to front\n",
    "* final dimensions must match\n",
    "\n",
    "E.g. adding `1` to `[1,2,3]` is the same as broadcasting `1` to `[1,1,1]` and adding `[1,1,1]` to `[1,2,3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3 into shape (3,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a7e2657998be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3 into shape (3,2)"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,4)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=\n",
      " [[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]]\n",
      "\n",
      " [[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]]]\n",
      "\n",
      "rank of y: 3\n",
      "\n",
      "shape of y: (2, 3, 10)\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(1,11).astype(float)\n",
    "y = np.ones((2, 3, 10)).astype(float)\n",
    "y[:]=x\n",
    "print('y=\\n',y)\n",
    "\n",
    "print('\\nrank of y:', y.ndim)\n",
    "\n",
    "print('\\nshape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_images =  (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('shape of train_images = ', train_images.shape)  #Note: above we have vectorised 28x28 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y = (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "y = np.reshape(train_images, (60000, 28,28)) #reshape into tensor of rank 3 (sample, height, width)\n",
    "print('shape of y =', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digit = y[34001,:,:]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
