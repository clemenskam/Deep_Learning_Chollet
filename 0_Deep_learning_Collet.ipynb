{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning: Chapter 2\n",
    "\n",
    "\n",
    "Example:  Classify handwritten digits\n",
    "\n",
    "* grayscale images, 28x28 pixels.  Sort these into 10 categories (digits 0 to 9)\n",
    "* [MNISt](https://en.wikipedia.org/wiki/MNIST_database) dataset: 60k training images, 10k test images\n",
    "\n",
    "\n",
    "Terminology:\n",
    "\n",
    "* **Class**: a category in a classification problem \n",
    "* **Sample**: datapoints\n",
    "* **Label**: class associatied with a specific sample\n",
    "\n",
    "MNIST database is preloaded in KERASin set of four Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\nDownloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11493376/11490434 [==============================] - 3s 0us/step\n"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `train_images` and `train_labels` from the training set\n",
    "* `test_images` and `test_labels` is test set on which model will be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Shape train_image = (60000, 28, 28)\nLength train_labels = 60000\nShape test_images= (10000, 28, 28)\nLength test_labels = 10000\n"
    }
   ],
   "source": [
    "print(f\"Shape train_image = {train_images.shape}\")\n",
    "print(f\"Length train_labels = {len(train_labels)}\")\n",
    "print(f\"Shape test_images= {test_images.shape}\")\n",
    "print(f\"Length test_labels = {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow: \n",
    "\n",
    "1. feed neuronal network training data\n",
    "2. associate images with labels\n",
    "3. produce predictions for test images\n",
    "4. verify whether predictions match test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation = 'relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* layer: is a filter\n",
    "* layers extract representations of data fed into them.\n",
    "* layers are chained\n",
    "\n",
    "Network above consists of a sequence of two dense layers that are *densely* connected (or *fully* connected) neural layers. The second layer is a 10 way *softmax* layer which returns 10 probability scores (summing to 1). Each score will give probability that current digit belongs to one of the 10 digit classes.\n",
    "\n",
    "To make the network ready for training need to pick three more things as part of the *compilation* step: \n",
    "\n",
    "* an optimizer (method to update network)\n",
    "* a loss function (how network measures performance on training data)\n",
    "* metrics to monitor during training and testing (here: accuracy of classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A representation of the work flow is below:\n",
    "\n",
    "<img src=\"https://dpzbhybb2pdcj.cloudfront.net/chollet/Figures/01fig09.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reshape images into a `float32` array of shape (60000, 28*28)\n",
    "* rescale 8 bit images into values in interval `[0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 *28))\n",
    "test_images = test_images.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorically encode labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train network.  Call network's fit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /Users/clemens/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nEpoch 1/5\n60000/60000 [==============================] - 2s 37us/step - loss: 0.2544 - accuracy: 0.9261\nEpoch 2/5\n60000/60000 [==============================] - 2s 33us/step - loss: 0.1046 - accuracy: 0.9695\nEpoch 3/5\n60000/60000 [==============================] - 2s 36us/step - loss: 0.0699 - accuracy: 0.9793\nEpoch 4/5\n60000/60000 [==============================] - 2s 33us/step - loss: 0.0513 - accuracy: 0.9843\nEpoch 5/5\n60000/60000 [==============================] - 2s 33us/step - loss: 0.0386 - accuracy: 0.9884\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7f80b837fe10>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs =5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* evaluate how good network is at categorising unknown data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10000/10000 [==============================] - 0s 29us/step\ntest_acc: 0.9787999987602234\n"
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is lower than the training set accuracy.  This is an example of *overfitting*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is in the workflow in short for generating a DL network in Keras. In what follows, we discuss what's behind the working.  We'll discuss data, tensors, tensor operations, data transoformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The types of data we are dealing with are:\n",
    "\n",
    "* Vector data: e.g. dataset of text documents where each document is represented by the counts of how many times each word in a dictionary appears in it.  e.g. tensor of shape `(500, 20000)` might describe 500 documents and the count for each of the words contained in a dictionary of 20000 words.\n",
    "\n",
    "* Time series data: e.g.stock prices as a tensor of rank 3: 250 days with values recorded every minute (390 working minutes per day), recording low, high and current price for each minute `(250, 390, 3)`\n",
    "\n",
    "* Image data: tensor of rank 4: e.g. 128 images, 256 x 256 pixels, 3 colour channels. `(128, 256, 256, 3)`. note Tensorflow convention for order is `(samples, height, width, color_depth)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor attributes\n",
    "\n",
    "* Number of axes (rank)\n",
    "* shape: dimension of tensor along each axis\n",
    "* Data type (usually `dtype` in python libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "shape of train_images =  (60000, 784)\n"
    }
   ],
   "source": [
    "print('shape of train_images = ', train_images.shape)  #Note: above we have vectorised 28x28 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "shape of y = (60000, 28, 28)\n"
    }
   ],
   "source": [
    "y = np.reshape(train_images, (60000, 28,28)) #reshape into tensor of rank 3 (sample, height, width)\n",
    "print('shape of y =', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(28, 28)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "digit = y[3]\n",
    "digit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a7df4e1e01ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdigit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m34001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "digit = y[34001]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing just bottom half of 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ace9f6557322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdigit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m34001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "digit = y[34001, 15:, :]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-82a107f3eb98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1011\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "start_index = 1011\n",
    "tiles = 7\n",
    "images = y[start_index:start_index+tiles]\n",
    "\n",
    "\n",
    "fig=plt.figure()\n",
    "for i in range(1,len(images)+1):\n",
    "    sub = fig.add_subplot(1, len(images), i)\n",
    "    sub.axis('off')\n",
    "    sub.imshow(images[i-1])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data batches\n",
    "\n",
    "the first axes (axis 0) is called the **sample axis**.  Usually the entire dataset can't be processed at once and one needs to break the dataset into smaller **batches**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_images' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2b6c360a96c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#batch n is from dataset[128*n:128*(n+1)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_images' is not defined"
     ]
    }
   ],
   "source": [
    "batch_1 = train_images[:128]\n",
    "\n",
    "batch_4 = train_images[128*4:128*(4+1)]  #batch n is from dataset[128*n:128*(n+1)] \n",
    "\n",
    "len(batch_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All transformations learned by DNN can be reduced to a small number of tensor operations (add, multiply, etc)\n",
    "\n",
    "E.g.\n",
    "```python\n",
    "network.add(layers.Dense(512, activation = 'relu', input_shape=(28 * 28,)))\n",
    "```\n",
    "\n",
    "this layer cab be interpreted as a function that takes a 2D tensor (vecor samples) and returns another 2D tensor.\n",
    "\n",
    "The function is:\n",
    "\n",
    "```python\n",
    "output = relu(dot(W, input) + b\n",
    "```\n",
    "\n",
    "* This includes a dot product between a 2D tensor W and an input tensor and adds a vector b. \n",
    "* It also includes a **rectified linear unit** function defined as `max(x, 0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified linear Unit - relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    '''Naive implementation of relu (rectified linear unit) function\n",
    "    takes tensor x of rank 2\n",
    "    ''' \n",
    "    assert len(x.shape) == 2, 'x has more than 2 axes, not an image!'\n",
    "    x = x.copy()    #produce a copy of x without overwriting input tensor\n",
    "   \n",
    "    \n",
    "    for i in range(0, x.shape[0]):\n",
    "        for j in range(0, x.shape[1]):\n",
    "            x[i, j]=max(x[i,j], 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9d25fe6a7477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#random.random only produces positive numbers in (0, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"x=\\n{x} \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Relu(x) = naive_relu(x) =\\n{naive_relu(x)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x = 2.0*(np.random.random((3,4))-0.5)  #random.random only produces positive numbers in (0, 1)\n",
    "\n",
    "print(f\"x=\\n{x} \\n\")\n",
    "print(f\"Relu(x) = naive_relu(x) =\\n{naive_relu(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same is achieved with the very simple inbuilt numpy method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7a75a39aed44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Relu(x) = np.maximum(x, 0) =\\n{np.maximum(x,0)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Relu(x) = np.maximum(x, 0) =\\n{np.maximum(x,0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_addition(x, y):\n",
    "    assert len(x.shape)== 2, \"Shape of x or y not rank 2!\"\n",
    "    assert x.shape == y.shape, \"Shape of x and y not same!\"\n",
    "    \n",
    "    x=x.copy()\n",
    "    y=y.copy()\n",
    "    \n",
    "    for i in range(0, x.shape[0]):\n",
    "        for j in range(0, x.shape[1]):\n",
    "            x[i,j]+=y[i,j]\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5b777491babd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"a =\\n{a}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"b =\\n{b}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(5, size=(2,3))\n",
    "print(f\"a =\\n{a}\")\n",
    "\n",
    "b = np.random.randint(5, size=(2,3))\n",
    "print(f\"b =\\n{b}\")\n",
    "\n",
    "print(f\"a + b = naive_addition(a, b) =\\n{naive_addition(a, b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "Means stretching dimensions of lower rank tensor to those of higher rank tensor for efficient computation.\n",
    "\n",
    "See [broadcasting article](https://numpy.org/devdocs/user/theory.broadcasting.html)\n",
    "\n",
    "* works from last axis backward to front\n",
    "* final dimensions must match\n",
    "\n",
    "E.g. adding `1` to `[1,2,3]` is the same as broadcasting `1` to `[1,1,1]` and adding `[1,1,1]` to `[1,2,3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1 2 3]\n"
    }
   ],
   "source": [
    "a = np.arange(1,4)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y=\n [[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]]\n\n [[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]]]\n\nrank of y: 3\n\nshape of y: (2, 3, 10)\n"
    }
   ],
   "source": [
    "x = np.arange(1,11).astype(float)\n",
    "y = np.ones((2, 3, 10)).astype(float)\n",
    "y[:]=x\n",
    "print('y=\\n',y)\n",
    "\n",
    "print('\\nrank of y:', y.ndim)\n",
    "\n",
    "print('\\nshape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-based optimisation\n",
    "\n",
    "each neural layer transforms input data as follows:\n",
    "\n",
    "```\n",
    "output = relu(dot(W, input) +b)\n",
    "```\n",
    "\n",
    "where W and b are tensors that are attributes of layers, called weights (*kernel*) or trainable parameters (*bias*) of the layer.  Initially, this does nothing useful, but gradually W and b are adjusted, based on feedback signal.  This adjustment takes place in a *training loop*.  Specifically\n",
    "\n",
    "1. Draw batch of training samples `x` and corresponding targets `y`\n",
    "1. Run network on `x` (called *forward pass*) to obain `y_predicted`\n",
    "1. Compute loss of network on batch, a measure of mismatch between `y_predicted` and `y`\n",
    "1. Update  weights of `W`, `b` so that loss is reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat, until loss is low: The network has now learned to map inputs to their correct targets.  \n",
    "* Step 1 is simple: just I/O code.\n",
    "* Steps 2 and 3 are merely application of a handful of tensor ops.\n",
    "* Step 4 is the difficult part.  How do we decide whether `W` and `b` coefficients should be increased or decreased? And by how much?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could do latter by freezing all coefficients except one, which you vary and recalculate loss.  If loss greater, change coefficient accordingly, etc.  Repeat sequentially for all coefficients. Problem: Terribly inefficient! There may be 10s or even 100s of thousands of coefficients to optimise: this makes problem intractable.  However: all operations described are differentiable. Better way: compute **gradient** of the loss wih regards to network coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivative:**\n",
    "\n",
    "Approximate `f` around a point `p` as:\n",
    "\n",
    "`f(x + epsilon_x) = y + a * epsilon_x`\n",
    "\n",
    "The slope `a` is called the derivative of `f` in `p`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector dot product\n",
    "\n",
    "\n",
    "$$ z = \\vec{x} \\cdot \\vec{y}$$\n",
    "\n",
    "where $z$ is a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x,y):\n",
    "    assert len(x.shape)==1, \"rank != 1: x not a vector!\"\n",
    "    assert len(y.shape)==1, \"rank != 1: y not a vector!\"\n",
    "    assert x.shape[0]==y.shape[0], \"len(x) != len(y): shapes incompatible!\"\n",
    "    \n",
    "    x=x.copy()\n",
    "    y=y.copy() #ensures that original x, y not overridden\n",
    "    z=0.0\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        z+=x[i]*y[i]\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1 2 3].[4 5 6]=\n 32.0\n"
    }
   ],
   "source": [
    "x=np.array([1,2,3])\n",
    "y=np.array([4,5,6])\n",
    "\n",
    "print(f\"{x}.{y}=\\n {naive_vector_dot(x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matrix - vector product:\n",
    "\n",
    "$$ \\vec{z} = \\mathbf{x} \\cdot \\vec{y}$$\n",
    "\n",
    "where $\\mathbf{x}$ is a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vec_dot(x, y):\n",
    "    assert len(x.shape)==2, \"rank of x != 2: not a matrix!\"\n",
    "    assert len(y.shape)==1, \"rank of y != 1: not a vector!\"\n",
    "    assert x.shape[1]==y.shape[0], \"x_cols != y_rows: shapes not compatible!\"\n",
    "    \n",
    "    x=x.copy()\n",
    "    y=y.copy()\n",
    "    z=np.zeros(x.shape[0])\n",
    "               \n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[0]):\n",
    "            z[i]+= x[i, j]*y[j]\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1 2]\n [3 4]].[2 3]=\n [ 8. 18.]\n"
    }
   ],
   "source": [
    "x=np.array([[1,2], [3,4]])\n",
    "y=np.array([2,3])\n",
    "\n",
    "print(f\"{x}.{y}=\\n {naive_matrix_vec_dot(x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "better way: make use of the vector dot product and multiply row vectors of x with column y:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vec_dot2(x,y):\n",
    "    assert len(x.shape)==2, \"rank of x != 2: not a matrix!\"\n",
    "    assert len(y.shape)==1, \"rank of y != 1: not a vector!\"\n",
    "    assert x.shape[1]==y.shape[0], \"x_cols != y_rows: shapes not compatible!\"\n",
    "    \n",
    "    x=x.copy()\n",
    "    y=y.copy()\n",
    "    z=np.zeros(x.shape[0])\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        z[i]=naive_vector_dot(x[i,:], y)\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1 2]\n [3 4]].[2 3]=[ 8. 18.]\n"
    }
   ],
   "source": [
    "x=np.array([[1,2], [3,4]])\n",
    "y=np.array([2,3])\n",
    "\n",
    "print(f\"{x}.{y}={naive_matrix_vec_dot2(x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor dot product:\n",
    "\n",
    "$$ \\mathbf{z} = \\mathbf{x} \\cdot \\mathbf{y}$$\n",
    "\n",
    "where $\\mathbf{x}$ and $\\mathbf{y}$ ar shape-compatible  matrixices (i.e. `len(x_rows)=len(y_columns)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_tensor_dot_product(x, y):\n",
    "    assert len(x.shape)==2, \"Rank of x != 2\"\n",
    "    assert len(y.shape)==2, \"Rank of y != 2\"\n",
    "    assert x.shape[0]==y.shape[1], \"columns of x don't match rows of y!\"\n",
    "    \n",
    "    x=x.copy()\n",
    "    y=y.copy()\n",
    "    z=np.zeros((x.shape[0], y.shape[1]))\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            z[i,j]=naive_vector_dot(x[i,:], y[:, j])\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1 2]\n [3 4]]\n.\n[[1 2]\n [3 4]]\n=\n[[ 7. 10.]\n [15. 22.]]\n"
    }
   ],
   "source": [
    "x=np.array([[1,2], [3,4]])\n",
    "y=np.array([[1,2], [3,4]])\n",
    "print(f\"{x}\\n.\\n{y}\\n=\\n{naive_tensor_dot_product(x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapes of tensor products\n",
    "\n",
    "<img src=\"https://dpzbhybb2pdcj.cloudfront.net/chollet/Figures/02fig05.jpg\" width=400>\n",
    "\n",
    "The dot product between higher-dimensional tensors:\n",
    "\n",
    "$$(a,b) \\cdot (b,c) \\rightarrow (a, c)$$\n",
    "\n",
    "\n",
    "$$(a,b,c,d) \\cdot (d,) \\rightarrow (a, b, c)$$\n",
    "\n",
    "$$(a,b,c,d) \\cdot (d, e) \\rightarrow (a, b, c, e)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric interpretation of deep learning\n",
    "\n",
    "Think of it as a series of elementary tensor operations to transform representation of data into a format, where "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector addition: \n",
    "\n",
    "$$\\vec{a} + \\vec{b} = \\vec{c}$$\n",
    "\n",
    "this is an addition of two tensors of rank 1: \n",
    "\n",
    "<img src=\"https://dpzbhybb2pdcj.cloudfront.net/chollet/HighResolutionFigures/figure_2-8.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matrix rotation\n",
    "\n",
    "This is dot product of a matrix (tensor of rank 2) with a vector (tensor of rank 1):\n",
    "\n",
    "\n",
    "$$ \\mathbf{R} \\cdot \\vec{x} = \\vec{y}$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{R} = \n",
    "\\begin{bmatrix}\n",
    "\\cos x & \\sin x \\\\\n",
    "-\\sin x & \\cos x\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation} \n",
    "\\begin{bmatrix}\n",
    "\\cos \\theta & \\sin \\theta \\\\\n",
    "-\\sin \\theta & \\cos \\theta\n",
    "\\end{bmatrix} \n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "x' \\\\\n",
    "y'\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can think of neural networks of chains of geometric transformations (tensor operations) of the input data.  These transformations take place in a high-dimensional space, implemented as long, chained, series of simple individual steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A useful mental image of class decomposition by deep learning\n",
    "\n",
    "Imagine two sheets of coloured paper (red and blue) crumpled together into a ball. The crumpled ball represents the input data. How do we unfold it to separate the individual sheets (classes of data)?  ML is figuring out the procedure of transformations that perform this separation, very much like the sequential number of individual operations you'd perform to 'uncrumple' the pieces of paper by hand, picking a corner at a time, stretching, unfolding, shearing, etc.:\n",
    "\n",
    "\n",
    "<img src=\"https://dpzbhybb2pdcj.cloudfront.net/chollet/HighResolutionFigures/figure_2-9.png\" width=600>\n",
    "\n",
    "This is what ML is about: finding neat representations for complex, highly folded data manifolds. The complex transformation required to achieve this is broken down into  a series of simple elementary ones - similar to what you would do to unfold the crumpled pieces of paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-based optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neural layer transforms its input data according to\n",
    "\n",
    "```python\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "\n",
    "Here `W` and `b` are tensors that are attributes of the layer. They are the *weights* and *trainable parameter* of the layer (also called `kernel` and `bias`), respectively.\n",
    "\n",
    "Initially these are assigned random values. Then they are gradually adjusted (*training*) based on a feedback signal. \n",
    "\n",
    "Steps:\n",
    "\n",
    "* draw a batch of training samples `x` and targets `y`\n",
    "* Run network on `x` (forward pass) yo obtain `y_predict`\n",
    "* compute loss from mismatch of `y_predict` and `y`\n",
    "* update weights and bias in network\n",
    "\n",
    "Last part hardest.  Could adjust one parameter, freeze all others: takes for ever.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better approach: network operations are differentiable.  Compute **gradient of loss** with regard to network coefficients.\n",
    "\n",
    "Steps:\n",
    "\n",
    "* draw a batch of training samples `x` and targets `y`\n",
    "* Run network on `x` (forward pass) yo obtain `y_predict`\n",
    "* compute loss from mismatch of `y_predict` and `y`\n",
    "* compute gradient of the loss with regard to network's parameters (a **backward pass**)\n",
    "* Move the parameters a little in opposite direction from the gradient.  E.g. \n",
    "```python\n",
    "W -= step * gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a *mini-batch stochastic gradient descent* (minibatch SGD). Stochastic becuse each batch of data drawn at radnom.  Figure below shows SGD down a 1D loss curve (one learnable parameter)\n",
    "\n",
    "<img src=\"https://dpzbhybb2pdcj.cloudfront.net/chollet/HighResolutionFigures/figure_2-11.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "    \n",
    "* step sizes (**learning rate**) need to be right size.  Too small: takes to long and you migth get stuck in local minimum. Too big: miss minimum.\n",
    "* Variations: *True SGD*: draw single sample.  *Minibatch SGD*: draw a batch.  *Batch SGD*: run on all available data.\n",
    "* Weights ```W``` operate on high-dimensional space.  Could have as many as 100000 dimensions or more.\n",
    "* Variants of SGD update `W` in different ways, e.g. SGD with momentum, Adagrad, RMSProp, etc.  \n",
    "\n",
    "<img src=\"https://dpzbhybb2pdcj.cloudfront.net/chollet/Figures/02fig12.jpg\" width = 400>\n",
    "\n",
    "Example of SGD down a 2D loss surface (2 learnable parameters). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help get you over and out of local minima, concept of *momentum* is useful. Think of ball rolling down the surface.  Momentum too small: gets stuck in local minimum.  With enough momentum gets over and out of local minumum until it hits global minimum. Base momentum on current slope value and also previous parameter update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "past_velocity = 0.\n",
    "momentum = 0.1 \n",
    "while loss < 0.01: #Optmimisation loop\n",
    "    w, loss, gradient= get_current_parameters()\n",
    "    w = w + momentum * velocity - learning_rate * gradient\n",
    "    past_velocity = velocity\n",
    "    update_parameter(w)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, eneural network function consists of may tensor operations chained together, each of which has a simple, known derivativ.  E,g, consider network $f$, composed of 3 tensor operations $\\mathbf{a}$, $\\mathbf{b}$, and $\\mathbf{c}$ with weight matrices $\\mathbf{W1}$, $\\mathbf{W2}$,$\\mathbf{W3}$:\n",
    "\n",
    "$$f(\\mathbf{W1, W2, W3}) = \\mathbf{a} (\\mathbf{W1, b}( \\mathbf{W2, c} (\\mathbf{W3})))$$\n",
    "\n",
    "chain rule:\n",
    "\n",
    "$$\\frac{d}{dx}f(g(x)) = f'(g(x))*g'(x)$$\n",
    "\n",
    "applying chain rule to neural network gives rise to an algorithm called **backpropagation**, or *reverse-mode differentiation*.  You start with final loss value and work backwards from top layers to bottom layers, applying the chain rule to compute contribution that each parameter had in loss value.  \n",
    "\n",
    "Tensor flow can do symbolic differentiation so gradient function becomes explicitly available. This means that Backpropagation algorithm simply becomes a call to this function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Learning means finding a combination of model parameters that minimizes a loss function for a given set of training data samples and their corresponding targets.\n",
    "* Learning happens by drawing random batches of data samples and their targets, and computing the gradient of the network parameters with respect to the loss on the batch. The network parameters are then moved a bit (the magnitude of the move is defined by the learning rate) in the opposite direction from the gradient.\n",
    "* The entire learning process is made possible by the fact that neural networks are chains of differentiable tensor operations, and thus it’s possible to apply the chain rule of derivation to find the gradient function mapping the current parameters and current batch of data to a gradient value.\n",
    "* Two key concepts you’ll see frequently in future chapters are loss and optimizers. These are the two things you need to define before you begin feeding data into a network.\n",
    "* The loss is the quantity you’ll attempt to minimize during training, so it should represent a measure of success for the task you’re trying to solve.\n",
    "* The optimizer specifies the exact way in which the gradient of the loss will be used to update parameters: for instance, it could be the RMSProp optimizer, SGD with momentum, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}